name: CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  DOCKER_IMAGE: refactool
  DOCKER_TAG: ${{ github.sha }}
  REDIS_HOST: localhost
  REDIS_PORT: 6379
  PYTHONPATH: ${{ github.workspace }}

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      redis:
        image: redis:latest
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.8'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-timeout pytest-asyncio
        
    - name: Start Celery worker
      run: |
        celery -A api.src.tasks worker --loglevel=info &
        sleep 5
        
    - name: Run unit tests
      run: |
        pytest -v -m "unit" --junitxml=test-reports/unit-tests.xml
        
    - name: Run integration tests  
      run: |
        pytest -v -m "integration" --junitxml=test-reports/integration-tests.xml
        
    - name: Run system tests
      run: |
        pytest -v -m "system" --junitxml=test-reports/system-tests.xml
        
    - name: Run stress tests
      run: |
        pytest -v -m "stress" --junitxml=test-reports/stress-tests.xml
        
    - name: Upload test results
      uses: actions/upload-artifact@v2
      if: always()
      with:
        name: test-results
        path: test-reports/
        retention-days: 7

  quality:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.8'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install black flake8
      
      - name: Format code
        run: |
          black .
          git config --global user.name 'GitHub Actions'
          git config --global user.email 'actions@github.com'
          git commit -am "style: Format code with Black" || true
      
      - name: Run linter
        run: |
          flake8 . \
            --count \
            --max-line-length=120 \
            --extend-ignore=E203,W503,E402,F401 \
            --statistics

  security:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.8'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install bandit
      
      - name: Run security scan
        run: |
          bandit -r . -f json -o security-report.json -ll -ii -s B101,B104,B105,B108 || true
      
      - name: Upload security report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-report
          path: security-report.json
          retention-days: 7

  publish:
    needs: [test, quality, security]
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.8'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install build twine
      
      - name: Build package
        run: python -m build
      
      - name: Publish to Test PyPI
        env:
          TWINE_USERNAME: __token__
          TWINE_PASSWORD: ${{ secrets.TEST_PYPI_TOKEN }}
        run: |
          twine upload --repository testpypi dist/*
      
      - name: Publish to PyPI
        if: github.event_name == 'push' && startsWith(github.ref, 'refs/tags')
        env:
          TWINE_USERNAME: __token__
          TWINE_PASSWORD: ${{ secrets.PYPI_TOKEN }}
        run: |
          twine upload dist/*

      - name: Discord Notification
        if: always()
        uses: sarisia/actions-status-discord@v1
        with:
          webhook: ${{ secrets.DISCORD_WEBHOOK }}
          title: "Package Published"
          description: |
            Version: ${{ github.ref }}
            Status: ${{ job.status }}
            Published to: ${{ github.event_name == 'push' && startsWith(github.ref, 'refs/tags') && 'PyPI' || 'Test PyPI' }}
          ack_no_webhook: true

  docker:
    needs: [test, quality, security]
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: |
            ${{ secrets.DOCKERHUB_USERNAME }}/${{ env.DOCKER_IMAGE }}:latest
            ${{ secrets.DOCKERHUB_USERNAME }}/${{ env.DOCKER_IMAGE }}:${{ env.DOCKER_TAG }}
          cache-from: type=registry,ref=${{ secrets.DOCKERHUB_USERNAME }}/${{ env.DOCKER_IMAGE }}:buildcache
          cache-to: type=registry,ref=${{ secrets.DOCKERHUB_USERNAME }}/${{ env.DOCKER_IMAGE }}:buildcache,mode=max

      - name: Discord Notification
        if: always()
        uses: sarisia/actions-status-discord@v1
        with:
          webhook: ${{ secrets.DISCORD_WEBHOOK }}
          title: "Docker Build"
          description: |
            Branch: ${{ github.ref }}
            Status: ${{ job.status }}
            Image: ${{ secrets.DOCKERHUB_USERNAME }}/${{ env.DOCKER_IMAGE }}:${{ env.DOCKER_TAG }}
          ack_no_webhook: true

  # Remover jobs que não estão sendo usados ainda
  # log_analysis: 